{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DevAssis/LLM_RAG_Dados_LangChaim/blob/master/LLM_RAG_dados_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color=blue> LLM's com RAG"
      ],
      "metadata": {
        "id": "UDOorrHcDhjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline para Converter Dados Não Estruturados em Perguntas e Respostas\n",
        "\n",
        "Este notebook implementa um pipeline de Processamento de Linguagem Natural (PNL) para transformar dados não estruturados em um sistema de perguntas e respostas interativo. O pipeline utiliza técnicas de embedding, armazenamento vetorial e modelos de linguagem avançados para permitir que os usuários obtenham insights a partir de seus dados.\n",
        "\n",
        "## Etapas do Pipeline\n",
        "\n",
        "1. **Carregamento de Dados**: Os dados não estruturados são carregados em um formato adequado para processamento.\n",
        "\n",
        "2. **Divisão do Texto**: O texto é dividido em unidades menores para facilitar a análise e o gerenciamento.\n",
        "\n",
        "3. **Embedding e Armazenamento**: As unidades de texto são convertidas em embeddings vetoriais e armazenadas em um banco de dados vetorial para busca eficiente.\n",
        "\n",
        "4. **Recuperação e Geração**: O sistema recupera as informações relevantes do banco de dados e gera respostas concisas e informativas para as perguntas dos usuários."
      ],
      "metadata": {
        "id": "emKJ0-ejOqzv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXD74S99Av52"
      },
      "source": [
        "# Etapa 1: Carregamento do Documento\n",
        "\n",
        "Nesta etapa, as lib's são instaladas e os dados não estruturados são carregados utilizando o LangChain. A fonte de dados pode variar, incluindo arquivos de texto, páginas da web, etc. O carregador selecionado converte os dados brutos em um objeto `Document` do LangChain, que será utilizado nas etapas subsequentes.\n",
        "\n",
        "**Exemplo:**\n",
        "\n",
        "\n",
        "```\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://bkjfiel.com.br/proverbios-1\")\n",
        "data = loader.load()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar o pacote atualizado\n",
        "!pip install -U langchain-openai"
      ],
      "metadata": {
        "id": "tuOq6edLDk5u",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "MAd1ISz7DwqU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar as bibliotecas necessárias\n",
        "!pip install -U langchain langchain-community openai faiss-cpu"
      ],
      "metadata": {
        "id": "HnosJlWJEMHr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "jtXqUR6dFgv5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "G4NJWAaoGhRt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhhAIDLXAv5y"
      },
      "source": [
        "##### Fetching API Key from Environment Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "sdXXTzp_a4D9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "19qFHR20Av50"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Fetch the OpenAI API key from environment variables\n",
        "api_key = os.environ.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Verificar se a chave da API foi recuperada corretamente\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Imprimir o resultado (por segurança, evite imprimir toda a chave publicamente)\n",
        "print(api_key is not None)  # Deve retornar True se a chave foi carregada corretamente\n"
      ],
      "metadata": {
        "id": "kz-uDs3ECE-I",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "if126NhRAv53",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "# Initialize the WebBaseLoader with the URL of the document to be loaded\n",
        "loader = WebBaseLoader(\"https://bkjfiel.com.br/proverbios-1\")\n",
        "\n",
        "# Load the document and store it in the 'data' variable\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ggEzzkuAv53",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Display the content of the loaded document\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pmn0IO3Av54"
      },
      "source": [
        "# Etapa 2: Dividindo o Documento em Partes\n",
        "\n",
        "O documento carregado é dividido em partes menores, chamadas de \"chunks\". Essa divisão facilita o processamento e a geração de embeddings. O tamanho dos chunks e a sobreposição entre eles são definidos para garantir que o contexto seja mantido.\n",
        "\n",
        "**Exemplo:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap = 20,\n",
        "    length_function = tiktoken_len\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(data)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hGnVBgIAv55",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "# Set up token encoding for the GPT-3.5 Turbo model\n",
        "tiktoken.encoding_for_model('gpt-4o-mini')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0hzRTD1Av55",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding('o200k_base')\n",
        "\n",
        "# Define a function to calculate the token length of a given text\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "tiktoken_len(\"O temor do Senhor é o princípio da Sabedoria.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZwxsvuuAv56"
      },
      "outputs": [],
      "source": [
        "# Este código inicializa um divisor de texto utilizando o `RecursiveCharacterTextSplitter`,\n",
        "# que divide o texto em partes (chunks) de tamanho especificado.\n",
        "# Neste caso, o tamanho do chunk é de 100 caracteres, com uma sobreposição de 20 caracteres entre os chunks.\n",
        "# A função `length_function` é usada para calcular o comprimento do texto usando `tiktoken_len`.\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Inicializar o divisor de texto com parâmetros especificados\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap = 20,\n",
        "    length_function = tiktoken_len\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtjOigdBAv56"
      },
      "outputs": [],
      "source": [
        "# Split the loaded document into smaller chunks\n",
        "chunks = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbAwWydDAv56",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Check the total number of chunks generated\n",
        "len(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH5naUKcAv57"
      },
      "source": [
        "# Etapa 3: Armazenando as Embeddings Vetoriais no Banco de Dados Vetorial\n",
        "\n",
        "Cada chunk de texto é convertido em uma representação vetorial (embedding) usando um modelo de embedding pré-treinado. Essas embeddings capturam o significado semântico do texto e permitem a busca por similaridade. As embeddings e os chunks são armazenados em um banco de dados vetorial (ChromaDB) para recuperação eficiente.\n",
        "\n",
        "**Exemplo:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs = {'device' : 'cpu'}\n",
        ")\n",
        "\n",
        "vectordb = Chroma.from_documents(chunks, hf)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUTp9qKkAv57",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# O código abaixo inicializa embeddings utilizando um modelo do HuggingFace.\n",
        "# O modelo utilizado é o \"sentence-transformers/all-MiniLM-L6-v2\", que é um modelo leve e eficiente\n",
        "# para gerar embeddings de sentenças.\n",
        "# O parâmetro `model_kwargs` especifica que o modelo será executado na CPU,\n",
        "# enquanto `encode_kwargs` define que as embeddings não serão normalizadas.\n",
        "# Após a configuração dos parâmetros, o HuggingFace Embeddings é inicializado.\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Especificar o nome do modelo e argumentos adicionais\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model_kwargs = {'device' : 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "# Inicializar as Embeddings do HuggingFace\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name = model_name,\n",
        "    model_kwargs = model_kwargs,\n",
        "    encode_kwargs = encode_kwargs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOFhQaAgAv57",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "embed = hf.embed_documents(texts=['h','e'])\n",
        "\n",
        "# Print the length of one of the embeddings to check its dimensions\n",
        "print(len(embed[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RloZO_QOAv58",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Install ChromaDB package using pip\n",
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLzRk67LAv58"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Initialize Chroma vector database with chunks and HuggingFace embeddings\n",
        "vectordb = Chroma.from_documents(chunks, hf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6P5dZBQAv58",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Perform a similarity search on the vector database\n",
        "vectordb.similarity_search('não mentir', k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBNxR9klAv59"
      },
      "source": [
        "# Etapa 4: Recuperar e Gerar\n",
        "\n",
        "Quando o usuário faz uma pergunta, o sistema busca no banco de dados vetorial os chunks mais relevantes (semelhantes à pergunta). Um modelo de linguagem (LLM) utiliza esses chunks e a pergunta do usuário para gerar uma resposta completa e informativa.\n",
        "\n",
        "**Exemplo:**\n",
        "\n",
        "\n",
        "```\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-4', temperature=0.6)\n",
        "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever())\n",
        "\n",
        "response = qa_chain({'query': 'Como devo viver minha vida na terra?'})\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp2WG6xtAv59",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Recuperar a chave da OpenAI da aba Secrets\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Verificar se a chave foi carregada corretamente\n",
        "if not api_key:\n",
        "    raise ValueError(\"Chave da API OpenAI não encontrada. Verifique se foi configurada corretamente na aba Secrets.\")\n",
        "\n",
        "# Inicializar o modelo de linguagem com a chave diretamente\n",
        "llm = ChatOpenAI(model_name='gpt-4', temperature=0.6, openai_api_key=api_key)\n",
        "\n",
        "# Inicializar a cadeia RetrievalQA com o modelo de linguagem e o retriever do banco de dados vetorial\n",
        "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever())\n",
        "\n",
        "# Passar uma consulta para a cadeia de QA para gerar uma resposta\n",
        "response = qa_chain({'query': 'Como devo viver minha vida na terra?'})\n",
        "\n",
        "# Exibir a resposta\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "# Exibir a resposta de forma mais organizada\n",
        "pprint.pprint(response)\n"
      ],
      "metadata": {
        "id": "DZZPAjYqKdpK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxHKsSPBAv59"
      },
      "outputs": [],
      "source": [
        "#Change the query to what you want to ask the LLM\n",
        "query = 'Como devo agir com as pessoas?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-hLxW_BAv59",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "qa_chain({'query' : query})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaCD70aoAv5-"
      },
      "source": [
        "# Próximos Passos\n",
        "\n",
        "* **Engenharia de Prompts**: Aperfeiçoar os prompts (instruções) fornecidos ao LLM para melhorar a qualidade e a relevância das respostas.\n",
        "\n",
        "* **Prompt Templates**: Utilizar Prompt Templates do LangChain para criar prompts dinâmicos e reutilizáveis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FIM"
      ],
      "metadata": {
        "id": "amUHZbbQLc5S"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}